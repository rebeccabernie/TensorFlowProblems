{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Flow Problem Sheet\n",
    "\n",
    "These problems relate to the [TensorFlow](https://www.tensorflow.org/) python library for pattern recognition. This notebook uses the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Use Tensorflow to create a model.\n",
    "Create a model that uses a flower's sepal width / length and petal width / length to predict the species of Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows: \n",
      "  [[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Importing here, gives errors if imported in part 4?\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "# SciKit (http://scikit-learn.org/stable/index.html) has good functionality for dealing with datasets - \n",
    "# http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
    "import sklearn.datasets as skds\n",
    "import sklearn.preprocessing as skpp\n",
    "\n",
    "# Load the dataset and print out the first 5 rows to make sure.\n",
    "iris = skds.load_iris()   # Inbuilt function - http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "print('First 5 rows: \\n ', iris.data[:5])\n",
    "\n",
    "x = iris.data # let x equal the full set of data in its original form (columns for sepal width/length etc)\n",
    "y_ = iris.target.reshape(-1, 1) # let y equal a single column of all data, for one hot encoding purposes\n",
    "\n",
    "# One Hot Encode - formats data to better fit classification algorithms in machine learning. See note.\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n",
    "encoder = skpp.OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_) # y is now the one hot encoded version of the dataset\n",
    "#print(y) # Uncomment the print statement to see better example of the table printed in the OneHotEncoding explaination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** While searching through examples of classification prediction in TensorFlow/Keras, I came across the term *One Hot Encoding* (OHE) a lot. After some online searching, I found that OHE is basically a way of transforming categorical features, such as plant type/classification, to a format that works better for machine learning algorithms. From my understanding, boolean columns are generated for each entry in the dataset - one column per type of entry. If an entry is a particular type, a `1` appears in the column, if not a `0`. For example, a randomised iris data set might look like this:\n",
    "\n",
    "|     | Setosa        | Versicolor    | Virginica  |\n",
    "|---- | ------------- |:-------------:|:-----:     |\n",
    "|Plant1     | 0      | 1 | 0     |\n",
    "|Plant2     | 1      | 0  |  0    |\n",
    "|Plant3     | 0 | 0     |  1     |\n",
    "\n",
    "In this table, Plant1 is a versicolor, Plant2 is a setosa, and Plant3 is a virginica.  \n",
    "See https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science for a slightly longer explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Actual building of the model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential() # Sequential models have a linear stack of layers\n",
    "\n",
    "# Dense = standard densely connected layer\n",
    "model.add(Dense(10, input_shape=(4,), activation='relu')) # Model will take as input arrays of shape (*, 4) and output arrays of shape (*, 10)\n",
    "model.add(Dense(10, activation='relu'))  # Don't need to specify input shape after first layer. relu = Rectified Linear Unit\n",
    "model.add(Dense(3, activation='softmax', name='output')) # Use softmax only in last layer because... see here https://stackoverflow.com/a/37601915/7232648\n",
    "\n",
    "# Adam optimizer with learning rate of 0.001 - Adam algorithm used when datasets have a seemingly random pattern\n",
    "# See https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ for further reading\n",
    "optimizer = Adam(lr=0.001) \n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy']) # Categorical crossentropy used for catergorical based datasets, like Iris\n",
    "\n",
    "print('Model Summary: ')\n",
    "print(model.summary()) # Prints a summary of the model - entire table below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the data into training and testing sets.\n",
    "Instructions: Investigate the *best way to do this* - write some code to randomly separate data if desired. Reference relevant material.\n",
    "\n",
    "I've decided to use the `train_test_split` function, from the `model_selection` class in SciKit Learn, because it handles spliting arrays into randomised subsets very simply. A full list of parameters can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split), but in this instance we'll use the following two:\n",
    "1. Arrays - a sequence of indexables with same length. In this case, `x` (original) and `y` (OHE).\n",
    "2. `test-size` - the size of the resulting test array. Option can be set to either a `float` between 0.0 and 1.0 to indicate the percentage of the original set to be used in the test set, or an int to indicate the number of entries to be used in the test set. Because the test size has been defined, we don't need to define the training size - the remaining percentage or entries will automatically be put into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:                            Test data:\n",
      "[ 5.7  2.8  4.1  1.3] [ 0.  1.  0.]       [ 5.4  3.9  1.7  0.4] [ 1.  0.  0.]\n",
      "[ 4.7  3.2  1.3  0.2] [ 1.  0.  0.]       [ 4.8  3.4  1.6  0.2] [ 1.  0.  0.]\n",
      "[ 6.1  2.8  4.   1.3] [ 0.  1.  0.]       [ 4.8  3.   1.4  0.3] [ 1.  0.  0.]\n",
      "[ 5.3  3.7  1.5  0.2] [ 1.  0.  0.]       [ 5.5  2.5  4.   1.3] [ 0.  1.  0.]\n",
      "[ 5.4  3.9  1.3  0.4] [ 1.  0.  0.]       [ 6.7  3.1  4.4  1.4] [ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection as skms # for splitting a set\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = skms.train_test_split(x, y, test_size=0.75)\n",
    "\n",
    "# Probably a better way of formatting this but will do for now.\n",
    "print(\"Training data:                           \", \"Test data:\")\n",
    "for i in range(5):\n",
    "    print('{0:} {1:} {2:} {3:} {4:}'.format(train_x[i], train_y[i],'     ', test_x[i], test_y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the original [Iris Data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data), the first row of the new training set above can be mapped to the 127th row in the original set, which is in fact virginica (indicated by a 1 in the third column of the OHE set above).  \n",
    "**Don't pay attention to this for now, sets randomise each time so data differs - will change on final commit.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model.\n",
    "Train the model using the test (should be training?) set.\n",
    "\n",
    "Using `model.fit` to train the model for a fixed number of epochs - see https://keras.io/models/sequential/ .  \n",
    "The `verbose` parameter can be set to either 0, 1 or 2. 0 means no output, 1 will display progress bars, and 2 will display one line per epoch. I've set it to 0 here for the sake of tidiness.  \n",
    "The `batch_size` parameter sets the number of samples per gradient update. A smaller number means a smaller loss result, but takes longer to run (depending on number of epochs/size of dataset) and will provide less accurate gradient descent. The default size is 32 - take the first 32 samples, train the neural network, take the next 32, retrain the NN, so on and so forth. Setting to 25 means the NN will be trained 3 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15e1730d400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, verbose=0, batch_size=25, epochs=500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model.\n",
    "Test your model using the testing set. Calculate and display clearly the error rate.  \n",
    "\n",
    "The `model.evaluate` function returns the loss value & metrics values (accuracy) for the model in test mode.  \n",
    "In general, the lower the loss, the better a model. High accuracy is obviously good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 97us/step\n",
      "\n",
      "Incorrect predictions: \n",
      " 8. Predicted: Versicolor \t Actual: Virginica \n",
      "17. Predicted: Versicolor \t Actual: Virginica \n",
      "25. Predicted: Virginica  \t Actual: Versicolor\n",
      "76. Predicted: Versicolor \t Actual: Virginica \n",
      "78. Predicted: [0 0 0]    \t Actual: Versicolor\n",
      "108. Predicted: Virginica  \t Actual: Versicolor\n",
      "\n",
      "Test Loss:     0.2808\n",
      "Test Accuracy: 95.5752%\n",
      "Error Rate:    4.4248%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rebec\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "results = model.evaluate(test_x, test_y) # first param is the test data, second param is the target data, i.e. the OHE set.\n",
    "i = 0\n",
    "wrong = 0\n",
    "for i in range (len(test_x)): # 75 goes to 74th entry, 0-74 = 75 entries...\n",
    "    # Prediction adapted from \"Predict class of single flower\" in https://github.com/emerging-technologies/keras-iris/blob/master/iris_nn.py\n",
    "    # np.around rounds the given param to given decimals, [0] = no decimals / whole number\n",
    "    prediction = np.around(model.predict(np.expand_dims(test_x[i], axis=0))).astype(np.int)[0]\n",
    "    \n",
    "    if prediction[0] == 1:\n",
    "        prediction = 'Setosa'\n",
    "    elif prediction[1] == 1:\n",
    "        prediction = 'Versicolor'\n",
    "    elif prediction[2] == 1:\n",
    "        prediction = 'Virginica'\n",
    "\n",
    "    if (test_y[i])[0] == 1:\n",
    "        actual = 'Setosa'\n",
    "    elif (test_y[i])[1] == 1:\n",
    "        actual = 'Versicolor'\n",
    "    elif (test_y[i])[2] == 1:\n",
    "        actual = 'Virginica'\n",
    "\n",
    "    if prediction != actual:\n",
    "        wrong = wrong + 1  # Add to number of incorrect predictions\n",
    "        if wrong == 1:\n",
    "            print(\"\\nIncorrect predictions: \") # Only print this heading the first time something goes wrong\n",
    "            \n",
    "        print(\"{:2}. Predicted: {:10} \\t Actual: {:10}\".format(i + 1, str(prediction), str(actual))) # Print the current incorrect prediction\n",
    "        \n",
    "# If no incorrect predictions...\n",
    "if wrong == 0:\n",
    "        print(\"No errors!\")\n",
    "        \n",
    "# evaluate function returns the loss and metrics/accuracy values as array,\n",
    "# results[0] is loss, results[1] is accuracy\n",
    "results[1] = results[1] * 100 # Get %\n",
    "error = 100 - results[1]\n",
    "\n",
    "# Output results\n",
    "print('\\nTest Loss:     {:0.4f}'.format(results[0]))\n",
    "print('Test Accuracy: {:0.4f}%'.format(results[1]))\n",
    "print('Error Rate:    {:0.4f}%'.format(error))\n",
    "\n",
    "#model.save(\"iris_model.h5\") # Save the model for future use, not completely necessary for a dataset of this size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
